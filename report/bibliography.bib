
@thesis{leggMachineSuperIntelligence2008,
  langid = {english},
  location = {{Lugano, Switzerland}},
  title = {Machine {{Super Intelligence}}},
  shorttitle = {Machine {{Superintelligence}}},
  pagetotal = {200},
  institution = {{University of Lugano}},
  type = {PhD Thesis},
  date = {2008},
  author = {Legg, Shane},
  file = {C\:\\Users\\DeadScholar\\Google Drive\\Zotero Files\\Legg_2008_Machine Super Intelligence.pdf}
}

@article{willemsContexttreeWeightingMethod1995,
  title = {The Context-Tree Weighting Method: Basic Properties},
  volume = {41},
  issn = {0018-9448},
  doi = {10/dcfdg9},
  shorttitle = {The Context-Tree Weighting Method},
  abstract = {Describes a sequential universal data compression procedure for binary tree sources that performs the "double mixture." Using a context tree, this method weights in an efficient recursive way the coding distributions corresponding to all bounded memory tree sources, and achieves a desirable coding distribution for tree sources with an unknown model and unknown parameters. Computational and storage complexity of the proposed procedure are both linear in the source sequence length. The authors derive a natural upper bound on the cumulative redundancy of the method for individual sequences. The three terms in this bound can be identified as coding, parameter, and model redundancy, The bound holds for all source sequence lengths, not only for asymptotically large lengths. The analysis that leads to this bound is based on standard techniques and turns out to be extremely simple. The upper bound on the redundancy shows that the proposed context-tree weighting procedure is optimal in the sense that it achieves the Rissanen (1984) lower bound.{$<>$}},
  number = {3},
  journaltitle = {IEEE Transactions on Information Theory},
  date = {1995-05},
  pages = {653-664},
  keywords = {Arithmetic,arithmetic codes,basic properties,binary sequences,binary tree sources,Binary trees,bounded memory tree sources,coding distributions,computational complexity,Context modeling,context-tree weighting method,cumulative redundancy,Data compression,Decoding,double mixture,Information theory,model redundancy,recursive weighting,redundancy,Redundancy,Rissanen lower bound,sequential universal data compression procedure,source coding,Source coding,source sequence length,State estimation,storage complexity,tree data structures,upper bound,Upper bound},
  author = {Willems, F. M. J. and Shtarkov, Y. M. and Tjalkens, T. J.},
  file = {C\:\\Users\\DeadScholar\\Google Drive\\Zotero Files\\Willems et al_1995_The context-tree weighting method.pdf},
  note = {ZSCC: 0000983}
}

@article{grunwaldTutorialIntroductionMinimum2004,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0406077},
  title = {A Tutorial Introduction to the Minimum Description Length Principle},
  url = {http://arxiv.org/abs/math/0406077},
  abstract = {This tutorial provides an overview of and introduction to Rissanen's Minimum Description Length (MDL) Principle. The first chapter provides a conceptual, entirely non-technical introduction to the subject. It serves as a basis for the technical introduction given in the second chapter, in which all the ideas of the first chapter are made mathematically precise. The main ideas are discussed in great conceptual and technical detail. This tutorial is an extended version of the first two chapters of the collection "Advances in Minimum Description Length: Theory and Application" (edited by P.Grunwald, I.J. Myung and M. Pitt, to be published by the MIT Press, Spring 2005).},
  urldate = {2019-07-25},
  date = {2004-06-04},
  keywords = {Computer Science - Machine Learning,Computer Science - Information Theory,Mathematics - Statistics Theory},
  author = {Grunwald, Peter},
  file = {C\:\\Users\\DeadScholar\\Google Drive\\Zotero Files\\Grunwald_2004_A tutorial introduction to the minimum description length principle.pdf},
  note = {ZSCC: 0000132}
}

@article{cilibrasiClusteringCompression2003,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cs/0312044},
  title = {Clustering by Compression},
  url = {http://arxiv.org/abs/cs/0312044},
  abstract = {We present a new method for clustering based on compression. The method doesn't use subject-specific features or background knowledge, and works as follows: First, we determine a universal similarity distance, the normalized compression distance or NCD, computed from the lengths of compressed data files (singly and in pairwise concatenation). Second, we apply a hierarchical clustering method. The NCD is universal in that it is not restricted to a specific application area, and works across application area boundaries. A theoretical precursor, the normalized information distance, co-developed by one of the authors, is provably optimal but uses the non-computable notion of Kolmogorov complexity. We propose precise notions of similarity metric, normal compressor, and show that the NCD based on a normal compressor is a similarity metric that approximates universality. To extract a hierarchy of clusters from the distance matrix, we determine a dendrogram (binary tree) by a new quartet method and a fast heuristic to implement it. The method is implemented and available as public software, and is robust under choice of different compressors. To substantiate our claims of universality and robustness, we report evidence of successful application in areas as diverse as genomics, virology, languages, literature, music, handwritten digits, astronomy, and combinations of objects from completely different domains, using statistical, dictionary, and block sorting compressors. In genomics we presented new evidence for major questions in Mammalian evolution, based on whole-mitochondrial genomic analysis: the Eutherian orders and the Marsupionta hypothesis against the Theria hypothesis.},
  urldate = {2019-07-25},
  date = {2003-12-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence,Condensed Matter - Statistical Mechanics,E4; H.3.3; H.5.5; I.2.6; I.2.10; I.5.3; J.3;J.5,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Genomics,Quantitative Biology - Quantitative Methods},
  author = {Cilibrasi, Rudi and Vitanyi, Paul},
  file = {C\:\\Users\\DeadScholar\\Google Drive\\Zotero Files\\Cilibrasi_Vitanyi_2003_Clustering by compression.pdf},
  note = {ZSCC: 0000011}
}

@article{venessMonteCarloAIXI2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0909.0801},
  primaryClass = {cs, math},
  title = {A {{Monte Carlo AIXI Approximation}}},
  url = {http://arxiv.org/abs/0909.0801},
  abstract = {This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. Our approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a new Monte-Carlo Tree Search algorithm along with an agent-specific extension to the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a variety of stochastic and partially observable domains. We conclude by proposing a number of directions for future research.},
  urldate = {2019-07-25},
  date = {2009-09-03},
  keywords = {Computer Science - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Information Theory},
  author = {Veness, Joel and Ng, Kee Siong and Hutter, Marcus and Uther, William and Silver, David},
  file = {C\:\\Users\\DeadScholar\\Google Drive\\Zotero Files\\Veness et al_2009_A Monte Carlo AIXI Approximation.pdf},
  note = {ZSCC: NoCitationData[s0]}
}

@article{downeyAlgorithmicRandomness2007,
  langid = {english},
  title = {Algorithmic Randomness},
  volume = {2},
  issn = {1941-6016},
  url = {http://www.scholarpedia.org/article/Algorithmic_randomness},
  doi = {10/d7ztfn},
  number = {10},
  journaltitle = {Scholarpedia},
  urldate = {2019-07-26},
  date = {2007-10-31},
  pages = {2574},
  keywords = {computational complexity,Computer Science - Information Theory,randomness},
  author = {Downey, Rodney G. and Reimann, Jan},
  file = {C\:\\Users\\DeadScholar\\Google Drive\\Zotero Files\\Algorithmic randomness - Scholarpedia_2019-07-26_07-18_b670ba57.mhtml},
  note = {ZSCC: 0000011}
}

@book{liIntroductionKolmogorovComplexity2019,
  langid = {english},
  title = {An {{Introduction}} to {{Kolmogorov}} Complexity and {{Its}} Applications},
  isbn = {978-3-030-11298-1},
  url = {https://doi.org/10.1007/978-3-030-11298-1},
  abstract = {"This must-read textbook presents an essential introduction to Kolmogorov complexity (KC), a central theory and powerful tool in information science that deals with the quantity of information in individual objects. The text covers both the fundamental concepts and the most important practical applications, supported by a wealth of didactic features. This thoroughly revised and enhanced fourth edition includes new and updated material on, amongst other topics, the Miller-Yu theorem, the Gács-Kučera theorem, the Day-Gács theorem, increasing randomness, short lists computable from an input string containing the incomputable Kolmogorov complexity of the input, the Lovász local lemma, sorting, the algorithmic full Slepian-Wolf theorem for individual strings, multiset normalized information distance and normalized web distance, and conditional universal distribution. Topics and features: Describes the mathematical theory of KC, including the theories of algorithmic complexity and algorithmic probability Presents a general theory of inductive reasoning and its applications, and reviews the utility of the incompressibility method Covers the practical application of KC in great detail, including the normalized information distance (the similarity metric) and information diameter of multisets in phylogeny, language trees, music, heterogeneous files, and clustering Discusses the many applications of resource-bounded KC, and examines different physical theories from a KC point of view Includes numerous examples that elaborate the theory, and a range of exercises of varying difficulty (with solutions) Offers explanatory asides on technical issues, and extensive historical sections Suggests structures for several one-semester courses in the preface As the definitive textbook on Kolmogorov complexity, this comprehensive and self-contained work is an invaluable resource for advanced undergraduate students, graduate students, and researchers in all fields of science."--Publisher's website.},
  urldate = {2019-07-28},
  date = {2019},
  keywords = {Computer Science - Artificial Intelligence,computational complexity,Computer Science - Information Theory},
  author = {Li, Ming and Vitányi, P. M. B},
  file = {C\:\\Users\\DeadScholar\\Google Drive\\Zotero Files\\Li_Vitányi_2019_An Introduction to Kolmogorov complexity and Its applications.pdf},
  note = {ZSCC: NoCitationData[s0] 
OCLC: 1106165074}
}

@book{liIntroductionKolmogorovComplexity2008a,
  location = {{New York}},
  title = {An Introduction to {{Kolmogorov}} Complexity and Its Applications},
  edition = {3rd ed},
  isbn = {978-0-387-33998-6 978-0-387-49820-1},
  pagetotal = {790},
  series = {Texts in Computer Science},
  publisher = {{Springer}},
  date = {2008},
  keywords = {Kolmogorov complexity},
  author = {Li, Ming and Vitányi, P. M. B.},
  file = {C\:\\Users\\DeadScholar\\Google Drive\\Zotero Files\\Li_Vitányi_2008_An introduction to Kolmogorov complexity and its applications.pdf},
  note = {ZSCC: 0006313}
}


